{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Risk Model of Emergency Admissions (ERMER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Initialise](#1.-Initialise)\n",
    "<br\\>\n",
    "[2. Read Data](#2.-Read-Data)\n",
    "<br\\>\n",
    "[3. Prepare Features](#3.-Prepare-Features)\n",
    "<br\\>\n",
    "[4. Build Sub-Models](#4.-Build-Sub-Models)\n",
    "<br\\>\n",
    "[5. Build Ensemble Model](#5.-Build-Ensemble-Model)\n",
    "<br\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter IPython Notebook applies the Ensemble Risk Model of Emergency Admissions (ERMER).\n",
    "\n",
    "The ERMER algorithm was developed as part of a PhD research at the <a href=\"http://www.healthcareanalytics.co.uk/\">Health &amp; Social Care Modelling Group (HSCMG)</a> at the <a href=\"http://www.westminster.ac.uk\">University of Westminster</a> (<a href=\"http://www.westminster.ac.uk\">Predictive Risk Modelling of Hospital Emergency Readmission,\n",
    "and Temporal Comorbidity Index Modelling Using Machine\n",
    "Learning Methods</a>).\n",
    "\n",
    "The published journal paper can be accessed via the following link: http://www.sciencedirect.com/science/article/pii/S1386505617300886\n",
    "\n",
    "Note: The script blocks in the Notebook must be carefully reviewed and configured before execution on a new problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr\\>\n",
    "<font size=\"1\" color=\"gray\">Copyright 2017, Mohsen Mesgarpour. All Rights Reserved.\n",
    "\n",
    "It is licensed under the Apache License, Version 2.0. you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "  <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a>\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</font>\n",
    "<hr\\>\n",
    "\n",
    "## 1. Initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules \n",
    "# It is an optional step. It can be useful to run when external Python modules are being modified\n",
    "# It is reloading all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "# Note: It may conflict with some python functions (like serialisation).\n",
    "\n",
    "# %load_ext autoreload \n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pprint\n",
    "from IPython.display import display, HTML\n",
    "from collections import OrderedDict\n",
    "from scipy.stats import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local Python modules\n",
    "from Stats.PreProcess import PreProcess\n",
    "from Stats.FeatureSelection import FeatureSelection\n",
    "from Stats.TrainingMethod import TrainingMethod\n",
    "from Stats.Plots import Plots\n",
    "from Configs.CONSTANTS import CONSTANTS\n",
    "from Configs.Logger import Logger\n",
    "from ReadersWriters.ReadersWriters import ReadersWriters\n",
    "from EnsembleModel.EnsembleModel import EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the interpreter\n",
    "print(\"\\nMake sure the correct Python interpreter is used!\")\n",
    "print(sys.version)\n",
    "print(\"\\nMake sure sys.path of the Python interpreter is correct (the ERMER project main folder))!\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.  Initialise General Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"font-weight:bold;color:red\">Main configuration Settings: </font>\n",
    "- Specify the full path of the configuration file \n",
    "<br/>&#9; &#8594; config_path\n",
    "- Specify the full path of the output folder \n",
    "<br/>&#9; &#8594; output_path\n",
    "- Specify the application name (the suffix of the outputs file name) \n",
    "<br/>&#9; &#8594; app_name\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<font style=\"font-weight:bold;color:red\">External Configration Files: </font>\n",
    "- The full path of the feature configuration file:\n",
    "<br/>&#9; &#8594; <i>ConfigInputs/input_features_configs</i>\n",
    "- TThe full path of the configuration file:\n",
    "<br/>&#9; &#8594; <i>ConfigInputs/CONFIGURATIONS</i>\n",
    "- The input features' confugration file (Note: only the CSV export of the XLSX will be used by this Notebook):\n",
    "<br/>&#9; &#8594; <i>ConfigInputs/input_features_configs.xlsx</i>\n",
    "<br/>&#9; &#8594; <i>ConfigInputs/input_features_configs.csv</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_features_path = os.path.abspath(\"ConfigInputs/input_features_configs\")\n",
    "config_path = os.path.abspath(\"ConfigInputs/CONFIGURATIONS\")\n",
    "io_path = os.path.abspath(\"Outputs\")\n",
    "app_name = \"ERMER\"\n",
    "\n",
    "print(\"\\n The full path of the feature configuration file: \\n\\t\", config_features_path,\n",
    "      \"\\n The full path of the configuration file: \\n\\t\", config_path,\n",
    "      \"\\n The full path of the input and output folder: \\n\\t\", io_path,\n",
    "      \"\\n The application name (the suffix of the outputs file name): \\n\\t\", app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(io_path):\n",
    "    os.makedirs(io_path, exist_ok=True)\n",
    "\n",
    "logger = Logger(path=io_path, app_name=app_name, ext=\"log\")\n",
    "logger = logging.getLogger(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise constants and some of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise constants        \n",
    "CONSTANTS.set(io_path, app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise other classes\n",
    "readers_writers = ReadersWriters()\n",
    "preprocess = PreProcess(io_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print settings\n",
    "pd.set_option('display.width', 1600, 'display.max_colwidth', 800)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.  Initialise Features Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input features' confugration file &amp; store the features metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables settings\n",
    "features_metadata = dict()\n",
    "\n",
    "features_metadata_all = readers_writers.load_csv(path=\"\", title=CONSTANTS.config_features_path, dataframing=True)\n",
    "features_metadata = features_metadata_all.loc[(features_metadata_all[\"Selected\"] == 1)]\n",
    "features_metadata.reset_index()\n",
    "    \n",
    "# print\n",
    "display(features_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set input features' metadata dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features names, and Dictionary of features types and dtypes\n",
    "features_names = []\n",
    "features_types = dict()\n",
    "features_dtypes = dict()\n",
    "\n",
    "for _, row in features_metadata.iterrows():\n",
    "    if row[\"Selected\"] == 1:\n",
    "        features_names.append(row[\"Variable_Name\"])\n",
    "        features_types[row[\"Variable_Name\"]] = row[\"Variable_Type\"]\n",
    "        features_dtypes[row[\"Variable_Name\"]] = row[\"Variable_dType\"]\n",
    "\n",
    "features_dtypes = pd.DataFrame(features_dtypes, index=[0]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of features groups\n",
    "features_types_group = OrderedDict()\n",
    "\n",
    "f_types = set([f_type for f_type in features_types.values()])\n",
    "features_types_group = OrderedDict(zip(list(f_types), [set() for _ in range(len(f_types))]))\n",
    "for f_name, f_type in features_types.items():\n",
    "    features_types_group[f_type].add(f_name)\n",
    "    \n",
    "print(\"Available features types: \" + ','.join(f_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Option I: Read from CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input features from the CSV input file\n",
    "- Specify the input folder path\n",
    "<br/>&#9; &#8594; input_path\n",
    "- Specify the input files names\n",
    "<br/>&#9; &#8594; input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.abspath(\"Samples\")\n",
    "input_files = dict()\n",
    "input_files[\"train\"] = dict({\"Age-65p_01\": \"sample__Cond_Age-65p_0__train\",\n",
    "                             \"Age-65p_1\": \"sample__Cond_Age-65p_1__train\",\n",
    "                             \"Main\": \"sample__Cond_Main__train\",\n",
    "                             \"Prior-Acute-12-month_0\": \"sample__Cond_Prior-Acute-12-month_0__train\",\n",
    "                             \"Prior-Acute-12-month_1\": \"sample__Cond_Prior-Acute-12-month_1__train\",\n",
    "                             \"Cond_Prior-Oper-12-month_0\": \"sample__Cond_Prior-Oper-12-month_0__train\",\n",
    "                             \"Cond_Prior-Oper-12-month_1\": \"sample__Cond_Prior-Oper-12-month_1__train\"})\n",
    "input_files[\"test\"] = dict({\"Age-65p_01\": \"sample__Cond_Age-65p_0__test\",\n",
    "                            \"Age-65p_1\": \"sample__Cond_Age-65p_1__test\",\n",
    "                            \"Main\": \"sample__Cond_Main__test\",\n",
    "                            \"Prior-Acute-12-month_0\": \"sample__Cond_Prior-Acute-12-month_0__test\",\n",
    "                            \"Prior-Acute-12-month_1\": \"sample__Cond_Prior-Acute-12-month_1__test\",\n",
    "                            \"Cond_Prior-Oper-12-month_0\": \"sample__Cond_Prior-Oper-12-month_0__test\",\n",
    "                            \"Cond_Prior-Oper-12-month_1\": \"sample__Cond_Prior-Oper-12-month_1__test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_input = dict({\"train\": {}, \"test\": {}})\n",
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        features_input[sample][submodel] = readers_writers.load_csv(path=input_path, title=input_files[sample][submodel], \n",
    "                                                                    dataframing=True)\n",
    "        print(\"Sample Name: \", sample, \n",
    "              \"; Submodel Name: \", submodel, \n",
    "              \"; Number of columns: \", len(features_input[sample][submodel].columns), \n",
    "              \"; Total records: \", len(features_input[sample][submodel].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Option II: Read from MySQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input features from the MySQL database\n",
    "- Specify the database name\n",
    "<br/>&#9; &#8594; db_schema\n",
    "- Specify the input table name\n",
    "<br/>&#9; &#8594; db_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_schema = \"\"\n",
    "# db_tables = dict()\n",
    "# db_tables[\"train\"] = dict({\"Age-65p_01\": \"sample__Cond_Age-65p_0__train\",\n",
    "#                            \"Age-65p_1\": \"sample__Cond_Age-65p_1__train\",\n",
    "#                            \"Main\": \"sample__Cond_Main__train\",\n",
    "#                            \"Prior-Acute-12-month_0\": \"sample__Cond_Prior-Acute-12-month_0__train\",\n",
    "#                            \"Prior-Acute-12-month_1\": \"sample__Cond_Prior-Acute-12-month_1__train\",\n",
    "#                            \"Cond_Prior-Oper-12-month_0\": \"sample__Cond_Prior-Oper-12-month_0__train\",\n",
    "#                            \"Cond_Prior-Oper-12-month_1\": \"sample__Cond_Prior-Oper-12-month_1__train\"})\n",
    "# db_tables[\"test\"] = dict({\"Age-65p_01\": \"sample__Cond_Age-65p_0__test\",\n",
    "#                           \"Age-65p_1\": \"sample__Cond_Age-65p_1__test\",\n",
    "#                           \"Main\": \"sample__Cond_Main__test.\",\n",
    "#                           \"Prior-Acute-12-month_0\": \"sample__Cond_Prior-Acute-12-month_0__test\",\n",
    "#                           \"Prior-Acute-12-month_1\": \"sample__Cond_Prior-Acute-12-month_1__test\",\n",
    "#                           \"Cond_Prior-Oper-12-month_0\": \"sample__Cond_Prior-Oper-12-month_0__test\",\n",
    "#                           \"Cond_Prior-Oper-12-month_1\": \"sample__Cond_Prior-Oper-12-month_1__test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_input = dict({\"train\": {}, \"test\": {}})\n",
    "# for sample in input_files.keys():\n",
    "#     for submodel in input_files[sample].keys():\n",
    "#         features_input[sample][submodel] = readers_writers.load_mysql_table(db_schema, db_tables[sample][submodel], \n",
    "#                                                                             dataframing=True)\n",
    "#         print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel, \n",
    "#               \"; Number of columns: \", len(features_input[sample][submodel].columns), \n",
    "#               \"; Total records: \", len(features_input[sample][submodel].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set Features Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        for col_name in features_input[sample][submodel].columns:\n",
    "            if col_name not in features_dtypes.keys():\n",
    "                logger.error(\"Invalid column in the input file: \" + str(col_name))\n",
    "        \n",
    "        col_names = set(features_input[sample][submodel].columns)\n",
    "        for col_name in features_dtypes.keys():\n",
    "            if col_name not in col_names:\n",
    "                logger.error(\"Missing column in the input file: \" + str(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        features_input[sample][submodel].astype(dtype=features_dtypes, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a descriptive stat report of 'Categorical', 'Continuous', & 'TARGET' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "        o_stats = preprocess.stats_discrete_df(df=features_input[sample][submodel], \n",
    "                                               includes=features_types_group[\"CATEGORICAL\"],\n",
    "                                               file_name=\"Stats_Categorical_\" + sample + \"_\" + submodel)\n",
    "        o_stats = preprocess.stats_continuous_df(df=features_input[sample][submodel], \n",
    "                                                 includes=features_types_group[\"CONTINUOUS\"], \n",
    "                                                 file_name=\"Stats_Continuous_\" + sample + \"_\" + submodel)\n",
    "        o_stats = preprocess.stats_discrete_df(df=features_input[sample][submodel], \n",
    "                                               includes=features_types_group[\"TARGET\"], \n",
    "                                               file_name=\"Stats_Target_\" + sample + \"_\" + submodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build Sub-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path of the Infer.Net libarary. It will be used to set local path variables when calling Infer.Net functions.\n",
    "\n",
    "More: <a href=\"http://infernet.azurewebsites.net/docs/Infer.NET%20Learners%20-%20Matchbox%20recommender%20-%20Command-line%20runners.aspx\">the Infer.NET command-line runners</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdLearner = \"set \\\"PATH=%PATH%;\" + os.path.abspath(\"Libraries/Infer.NET_2.6/Bin/\") + \"\\\" && Learner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Prepare Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the selected features for submodels into CSV files, to be used for training and testing using Infer.Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        readers_writers.save_csv(data=features_input[sample][submodel].drop(['hesid'], axis=1), \n",
    "                                 path=io_path, title=input_files[sample][submodel] + \"_InferDotNet\", append=False, ext=\"csv\", \n",
    "                                 header=features_input[sample][submodel].drop(['hesid'], axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the CSV to correct format, using the Infer.Net <a href=\"http://infernet.azurewebsites.net/docs/Infer.NET%20Learners%20-%20Matchbox%20recommender%20-%20Command-line%20runners.aspx\">guideline</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in input_files.keys():\n",
    "    for submodel in input_files[sample].keys():\n",
    "        # execute shell commands\n",
    "        cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "        cmd2 = os.path.abspath(\"ReadersWriters\\script_vim.txt\")\n",
    "        !vim $cmd1 -S $cmd2\n",
    "        \n",
    "        cmd1 = os.path.join(io_path, \".\" + input_files[sample][submodel] + \"_InferDotNet.csv.un~\")\n",
    "        !DEL $cmd1\n",
    "        cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv~\")\n",
    "        !DEL $cmd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train submodels (using train sub-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_model.mdl\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine Train --iterations 30 --batches 1 --training-set $cmd1 --model $cmd2  --compute-evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Predict the Train Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test submodels using train sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[\"train\"][submodel] + \"_evaluate_model.mdl\")\n",
    "    cmd3 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_predictions.predictions\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine Predict --test-set $cmd1 --model $cmd2 --predictions $cmd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute the perfromance of the predictions on train sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_predictions.predictions\")\n",
    "    cmd3 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_Report.txt\")\n",
    "    cmd4 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_CalibrationCurve.csv\")\n",
    "    cmd5 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_ROC.csv\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier Evaluate --ground-truth $cmd1 --predictions $cmd2 --report $cmd3 --calibration-curve $cmd4 --roc-curve $cmd5  --positive-class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Predict the Test Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test submodels using test sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"test\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[\"train\"][submodel] + \"_evaluate_model.mdl\")\n",
    "    cmd3 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_predictions.predictions\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine Predict --test-set $cmd1 --model $cmd2 --predictions $cmd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute the perfromance of the predictions on test sub-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"test\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_predictions.predictions\")\n",
    "    cmd3 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_Report.txt\")\n",
    "    cmd4 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_CalibrationCurve.csv\")\n",
    "    cmd5 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_ROC.csv\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier Evaluate --ground-truth $cmd1 --predictions $cmd2 --report $cmd3 --calibration-curve $cmd4 --roc-curve $cmd5  --positive-class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Weight Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the posterior weight distribution of features for the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_evaluate_model.mdl\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_report_weights.txt\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine SampleWeights  --model $cmd1 --samples $cmd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Diagnose Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the convergence of the message-passing algorithms used to train the Bayes Point Machine classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_report_diagnoseTrain.csv\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine DiagnoseTrain --iterations 500 --batches 1 --training-set $cmd1  --results $cmd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Cross-Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the generalization performance of the Bayes Point Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"train\"\n",
    "for submodel in input_files[sample].keys():\n",
    "    print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "    cmd1 = os.path.join(io_path, input_files[sample][submodel] + \"_InferDotNet.csv\")\n",
    "    cmd2 = os.path.join(io_path, input_files[sample][submodel] + \"_report_crossValidation.csv\")\n",
    "    \n",
    "    # execute shell command\n",
    "    !$cmdLearner Classifier BinaryBayesPointMachine CrossValidate --folds 5 --iterations 30 --batches 1 --data-set $cmd1 --results $cmd2 --compute-evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Initialise Labels & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict()\n",
    "ids = dict()\n",
    "ids_all = set()\n",
    "\n",
    "for sample in input_files.keys():\n",
    "    ids[sample] = dict()\n",
    "    for submodel in input_files[sample].keys():\n",
    "        print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "        ids[sample][submodel] = dict(zip(features_input[sample][submodel]['hesid'], \n",
    "                                         range(1, len(features_input[sample][submodel]['hesid']))))\n",
    "        ids_all = ids_all.union(set(features_input[sample][submodel]['hesid']))\n",
    "        for i in range(len(features_input[sample][submodel]['hesid'])):\n",
    "            labels[features_input[sample][submodel]['hesid'][i]] = features_input[sample][submodel]['label'][i]\n",
    "\n",
    "ids_all = list(ids_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted probailisites of label-1 (to be readmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dict()\n",
    "\n",
    "for sample in input_files.keys():\n",
    "    predictions[sample] = dict()\n",
    "    for submodel in input_files[sample].keys():\n",
    "        print(\"Sample Name: \", sample, \"; Submodel Name: \", submodel)\n",
    "        report = readers_writers.load_csv(path=os.path.join(io_path), \n",
    "                                          title=input_files[sample][submodel] + \"_evaluate_predictions\",\n",
    "                                          ext=\"predictions\",\n",
    "                                          dataframing=False)\n",
    "        predictions[sample][submodel] = [float((row[0].split(' ')[0]).split('=')[1]) for row in report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine labels and predictions for all the sample instances, including the instances that are not in submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all = dict()\n",
    "labels_all = dict()\n",
    "\n",
    "for sample in input_files.keys():\n",
    "    predictions_all[sample] = dict()\n",
    "    labels_all[sample] = dict()\n",
    "    for submodel in input_files[sample].keys():\n",
    "        predictions_all[sample][submodel] = []\n",
    "        labels_all[sample][submodel] = []\n",
    "        for i in ids_all:\n",
    "            labels_all[sample][submodel].append(labels[i])\n",
    "            if i in ids[sample][submodel].keys():\n",
    "                predictions_all[sample][submodel].append(predictions[sample][submodel][ids[sample][submodel][i]])\n",
    "            else:\n",
    "                predictions_all[sample][submodel].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Optimise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel(weight_sum_min=2,\n",
    "                               weight_sum_max=60,\n",
    "                               weight_max=59,\n",
    "                               trials_max=15,\n",
    "                               iteration_max=100,\n",
    "                               alpha_ensemble_min=0.005,\n",
    "                               alpha_model_min=0.5,\n",
    "                               ensemble_func_acc_weight=0.25,\n",
    "                               ensemble_func_auc_weight=0.25,\n",
    "                               ensemble_func_rmse_weight=0.25,\n",
    "                               ensemble_func_sar_weight=0.25,\n",
    "                               ensemble_type=\"mean\",\n",
    "                               output_path=os.path.join(io_path),\n",
    "                               output_name=\"ensemble_model_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the ensemble model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodels_names = list(input_files[sample].keys())\n",
    "submodels_names.sort()\n",
    "sample = \"train\"\n",
    "\n",
    "y = labels_all[sample][submodels_names[0]]\n",
    "y_hat_submodels = np.array([predictions_all[sample][submodel] for submodel in submodels_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the ensemble model optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.run(y=y,\n",
    "                   y_hat_submodels=y_hat_submodels,\n",
    "                   submodels_names=submodels_names,\n",
    "                   submodel_main=\"Main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. View Generated Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the summary output file that was generated by the ensemble model algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_full = os.path.join(io_path) + \"/ensemble_model_outputs.log\"\n",
    "print(\"Output file path: \", output_path_full)\n",
    "\n",
    "if sys.platform.startswith('darwin'):\n",
    "    subprocess.call(('open', output_path_full))\n",
    "elif os.name == 'nt':\n",
    "    os.startfile(output_path_full)\n",
    "elif os.name == 'posix':\n",
    "    subprocess.call(('xdg-open', output_path_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Configure an Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the weights for an ensemble model based on one of the generted ensemble models in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodels_names_func = {\"Age-65p_01\": 17, \"Age-65p_1\": 6, \"Cond_Prior-Oper-12-month_0\": 8,\n",
    "                        \"Cond_Prior-Oper-12-month_1\": 7, \"Main\": 6, \"Prior-Acute-12-month_0\": 12, \n",
    "                        \"Prior-Acute-12-month_1\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Assess Preformance for Train Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the predicted probabilities for the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ensemble_1 = ensemble_model.ensemble_y_hat(y_hat_submodels=y_hat_submodels,\n",
    "                                                 submodels_names=submodels_names,\n",
    "                                                 submodels_names_func=submodels_names_func,\n",
    "                                                 ensemble_type=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate statistics for the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_stats = ensemble_model.stat_report_full(y=y,\n",
    "                                               y_hat=y_hat_ensemble_1,\n",
    "                                               cut_off=0.5)\n",
    "print(\"Examine an ensemble model: \\n\" + str(submodels_names_func))\n",
    "pp.pprint(output_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Assess Preformance for Test Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the predicted probabilities for an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_ensemble_1 = ensemble_model.ensemble_y_hat(y_hat_submodels=y_hat_submodels,\n",
    "                                                 submodels_names=submodels_names,\n",
    "                                                 submodels_names_func=submodels_names_func,\n",
    "                                                 ensemble_type=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate statistics for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stats = ensemble_model.stat_report_full(y=y,\n",
    "                                               y_hat=y_hat_ensemble_1,\n",
    "                                               cut_off=0.5)\n",
    "print(\"Examine an ensemble model: \\n\" + str(submodels_names_func))\n",
    "pp.pprint(output_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
